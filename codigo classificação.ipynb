{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99d41377",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Library to Install\n",
    "#!pip install sklearn-genetic-opt\n",
    "#!pip install psutil\n",
    "#!pip install nvidia-ml-py3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from multiprocessing import Process, Queue\n",
    "import time\n",
    "\n",
    "\n",
    "# Importing the required libraries\n",
    "import psutil\n",
    "#import nvidia_smi\n",
    "\n",
    "# Creating an almost infinite for loop to monitor the details continuously\n",
    "def check_ressources(q, msg):\n",
    "    nvidia_smi.nvmlInit()\n",
    "    i = 0\n",
    "    cpu_usage = []\n",
    "    mem_usage = []\n",
    "    gpus = nvidia_smi.nvmlDeviceGetCount()\n",
    "    time = []\n",
    "    gpus_usage = {}\n",
    "    while(q.empty()):\n",
    "        # Obtaining all the essential details\n",
    "        cpu_usage.append(psutil.cpu_percent())\n",
    "        mem_usage.append(psutil.virtual_memory().percent)\n",
    "        for gpu in range(gpus):\n",
    "            handle = nvidia_smi.nvmlDeviceGetHandleByIndex(gpu)\n",
    "            info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "            if gpu in gpus_usage.keys():\n",
    "                gpus_usage[gpu].append((info.used/info.total)*100)\n",
    "            else:\n",
    "                gpus_usage[gpu] = [(info.used/info.total)*100]\n",
    "        time.append(i)\n",
    "        i = i + 50\n",
    "        plt.pause(0.05)\n",
    "    plt.xlabel(\"Time in ms\")\n",
    "    plt.ylabel(\"Percentage of usage\")\n",
    "    plt.plot(time, cpu_usage, color = \"red\", linestyle = 'dotted')\n",
    "    plt.plot(time, mem_usage, color = \"blue\", linestyle = 'dotted')\n",
    "    legend = [\"CPU\", \"Memory\"]\n",
    "    for gpu in gpus_usage:\n",
    "        legend.append(f'GPU Usage ID {gpu}')\n",
    "        plt.plot(time, gpus_usage[gpu], linestyle = 'dotted')\n",
    "    plt.legend(legend, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.title(f\"Ressource Plot: for {msg}\")\n",
    "    print(q.get())\n",
    "    plt.show()\n",
    "    nvidia_smi.nvmlShutdown()\n",
    "\n",
    "def auprc_multiclass(y_test, y_pred, classes):\n",
    "    yt = preprocessing.label_binarize(y_test, classes=classes)\n",
    "    yp = preprocessing.label_binarize(y_pred, classes=classes)\n",
    "    precision, recall, _ = metrics.precision_recall_curve(yt.ravel(),yp.ravel())\n",
    "    return metrics.auc(recall, precision)\n",
    "\n",
    "def plot_confusion_matrix(y_test, y_pred):\n",
    "    plt.figure(figsize=(12,10))\n",
    "    sns.set(font_scale=1.2)\n",
    "    sns.heatmap(\n",
    "      confusion_matrix(y_test, y_pred), \n",
    "      xticklabels=np.unique(y_pred), \n",
    "      yticklabels=np.unique(y_test), \n",
    "      annot=True,\n",
    "      annot_kws={\"size\": 12}, fmt='g')\n",
    "\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.show()\n",
    "\n",
    "class Malware_Detection:\n",
    "    def __init__(self, dataset=\"MalMen2020\", verbose=False, optimization=True, lines=-1):\n",
    "        if dataset == \"MalMen2020\":\n",
    "            dfs = [pd.read_csv(\"Obfuscated-MalMem2022.csv\")]\n",
    "        elif dataset == \"ADFA-LD\":\n",
    "            self.type_g = ['3-gramme', '5-gramme', '6-gramme']\n",
    "            dfs = [pd.read_csv(\"/kaggle/input/adfald/train_3g.csv\"),\n",
    "                   pd.read_csv(\"/kaggle/input/adfald/train_5g.csv\"),\n",
    "                   pd.read_csv(\"/kaggle/input/adfald/train_6g.csv\")]\n",
    "        else:\n",
    "            raise ValueError( \"Error: Unknow Dataset Please Choose either MalMen2020 or ADFA-LD\")\n",
    "        self.verbose = verbose\n",
    "        self.dataset = dataset\n",
    "        self.opt = optimization\n",
    "        self.df = None\n",
    "        print(f\"Preprocessing the Dataset {dataset} in progress ...\")\n",
    "        for df in dfs:\n",
    "            self.preprocessing(df)\n",
    "        print('Processing Done !')\n",
    "        if lines != -1:\n",
    "            print(f\"Only {lines} rows will be used from the Dataset {dataset}....\")\n",
    "            self.dataset_crop(lines)\n",
    "        self.output_evaluation = pd.DataFrame()\n",
    "    def dataset_crop(self, lines):\n",
    "        if self.dataset == \"MalMen2020\":\n",
    "            if lines >= len(self.df):\n",
    "                raise ValueError(f'Number of rows selected is {lines} >= number of rows in the dataset')\n",
    "            benign_dataset = self.df[self.df['Class'] == 'Benign']\n",
    "            total_rows = len(benign_dataset)\n",
    "            frac = ((lines/2 + 1)/total_rows)\n",
    "            benign_dataset = benign_dataset.sample(frac=frac)\n",
    "            self.df = self.df[self.df['Class'] != 'Benign']\n",
    "            malware_sub = self.df['Category_Subfamily'].unique()\n",
    "            nb_subs = len(malware_sub)\n",
    "            list_dataset = [benign_dataset]\n",
    "            for mal in malware_sub:\n",
    "                malware_dataset = self.df[self.df['Category_Subfamily'] == mal]\n",
    "                total_rows = len(malware_dataset)\n",
    "                frac = ((lines/2 + 1)/nb_subs)/total_rows\n",
    "                list_dataset.append(malware_dataset.sample(frac=frac))\n",
    "            self.df = pd.concat(list_dataset)\n",
    "        else:\n",
    "            for i,df in enumerate(self.df):\n",
    "                if lines >= len(df):\n",
    "                    raise ValueError(f'Number of rows selected is {lines} >= number of rows in the dataset')\n",
    "                benign_dataset = df[df['Label'] == 'Normal']\n",
    "                total_rows = len(benign_dataset)\n",
    "                frac = ((lines/2 + 1)/total_rows)\n",
    "                benign_dataset = benign_dataset.sample(frac=frac)\n",
    "                df = df[df['Label'] != 'Normal']\n",
    "                malware_sub = df['Label'].unique()\n",
    "                nb_subs = len(malware_sub)\n",
    "                list_dataset = [benign_dataset]\n",
    "                for mal in malware_sub:\n",
    "                    malware_dataset = df[df['Label'] == mal]\n",
    "                    total_rows = len(malware_dataset)\n",
    "                    frac = ((lines/2 + 1)/nb_subs)/total_rows\n",
    "                    list_dataset.append(malware_dataset.sample(frac=frac))\n",
    "                self.df[i] = pd.concat(list_dataset)\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "    def init_model(self,  model_name_binary='CART',model_name_family='CART', model_name_subfamily='CART'):\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        if self.dataset == \"MalMen2020\":\n",
    "            print(f\"Pipeline : Training for Malware/Benign Classification Model Used: {model_name_binary} <====> Malware Family Classification Model Used: {model_name_family} <===> Malware Sub Family Classification Model Used: {model_name_subfamily}\")\n",
    "            # Training for Malware/Benign Classification\n",
    "            X = self.df.copy(deep=True)\n",
    "            y_bin = X['Class']\n",
    "            X = X.drop(['Class', 'Category_Subfamily', 'Category_Family'], axis=1)\n",
    "            y_bin_ = y_bin.map({x:i for i,x in enumerate(y_bin.unique())})\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y_bin_, test_size=0.2, random_state=1) # 80% training and 20% test\n",
    "\n",
    "            #Ressource Check\n",
    "            q = Queue()\n",
    "            p = Process(target=check_ressources, args=(q, f\"Training for Malware/Benign Classification Model Used: {model_name_binary}\"))\n",
    "            p.start()\n",
    "            self.model_b = self.fit_model(model_name_binary, X_train, y_train)\n",
    "            q.put('Done')\n",
    "            p.join()\n",
    "\n",
    "            y_pred = self.model_b.predict(X_test)\n",
    "\n",
    "            y_pred = y_pred.astype(str)\n",
    "            y_test = y_test.astype(str)\n",
    "            for i,x in enumerate(y_bin.unique()):\n",
    "                y_pred[y_pred == str(i)] = x\n",
    "                y_test[y_test == str(i)] = x\n",
    "            print(f\"Evaluation of Malware/Benign Classification for {model_name_binary} Algorithm\")\n",
    "            self.evaluate_model(y_test, y_pred, y_bin.unique(), model_name_binary, \"Malware/Benign Classification\")\n",
    "\n",
    "            # Training for Malware Family Classification \n",
    "            X = self.df[self.df['Category_Family'] != 'Benign'].copy(deep=True)\n",
    "            y_f = X['Category_Family']\n",
    "            X = X.drop(['Class', 'Category_Subfamily', 'Category_Family'], axis=1)\n",
    "            y_f_ = y_f.map({x:i for i,x in enumerate(y_f.unique())})\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y_f_, test_size=0.2, random_state=1) # 80% training and 20% test\n",
    "\n",
    "            # Ressource Check\n",
    "            q = Queue()\n",
    "            p = Process(target=check_ressources, args=(q, f\"Malware Family Classification Model Used: {model_name_family}\"))\n",
    "            p.start()\n",
    "            self.model_f = self.fit_model(model_name_family, X_train, y_train)\n",
    "            q.put('Done')\n",
    "            p.join()\n",
    "\n",
    "\n",
    "\n",
    "            y_pred = self.model_f.predict(X_test)\n",
    "            y_pred = y_pred.astype(str)\n",
    "            y_test = y_test.astype(str)\n",
    "            for i,x in enumerate(y_f.unique()):\n",
    "                y_pred[y_pred == str(i)] = x\n",
    "                y_test[y_test == str(i)] = x\n",
    "\n",
    "            print(f\"Evaluation of Malware Family Classification for {model_name_family} Algorithm\")\n",
    "            self.evaluate_model(y_test, y_pred, y_f.unique(), model_name_family, \"Malware Family Classification\")\n",
    "\n",
    "            # Training for Malware Sub Family Classification\n",
    "            X = self.df[self.df['Category_Family'] != 'Benign'].copy(deep=True)\n",
    "            Xs = [X[X['Category_Family'] == f] for f in X['Category_Family'].unique()]\n",
    "            y_sfs = [x['Category_Subfamily'] for x in Xs]\n",
    "            for X, y_sf in zip(Xs, y_sfs):\n",
    "                fam = X['Category_Family'].unique()\n",
    "                X = X.drop(['Class', 'Category_Subfamily', 'Category_Family'], axis=1)\n",
    "                y_sf_ = y_sf.map({x:i for i,x in enumerate(y_sf.unique())})\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y_sf_, test_size=0.2, random_state=1) # 80% training and 20% test\n",
    "                # Ressource Check\n",
    "                q = Queue()\n",
    "                p = Process(target=check_ressources, args=(q, f\"Malware Sub Family Classification Model Used: {model_name_subfamily}\"))\n",
    "                p.start()\n",
    "                self.model_f = self.fit_model(model_name_subfamily, X_train, y_train)\n",
    "                q.put('Done')\n",
    "                p.join()\n",
    "                y_pred = self.model_f.predict(X_test)\n",
    "                y_pred = y_pred.astype(str)\n",
    "                y_test = y_test.astype(str)\n",
    "                for i,x in enumerate(y_sf.unique()):\n",
    "                    y_pred[y_pred == str(i)] = x\n",
    "                    y_test[y_test == str(i)] = x\n",
    "                print(f\"Evaluation of Malware Sub Family Classification for {model_name_subfamily} Algorithm\")\n",
    "                self.evaluate_model(y_test, y_pred, y_sf.unique(), model_name_subfamily, f\"Malware Sub Family Classification {fam[0]}\")\n",
    "        else:\n",
    "            for i,df in enumerate(self.df):\n",
    "                print(f\"Dataset Used : {self.type_g[i]}\\n\\n\")\n",
    "                print(f\"Pipeline : Training for Attack/Normal Classification Model Used: {model_name_binary} <====> Attacks Family Classification Model Used: {model_name_family}\")\n",
    "                # Training for Attack/Normal Classification\n",
    "                X = df.copy(deep=True)\n",
    "                y_bin = X['Label']\n",
    "                X = X.drop(['Label', 'ID'], axis=1)\n",
    "                y_bin_ = y_bin.map({x:1 if x != \"Normal\" else 0 for x in y_bin.unique()})\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y_bin_, test_size=0.2, random_state=1) # 80% training and 20% test\n",
    "\n",
    "                #Ressource Check\n",
    "                q = Queue()\n",
    "                p = Process(target=check_ressources, args=(q, f\"Training for Attack/Normal Classification Model Used: {model_name_binary}\"))\n",
    "                p.start()\n",
    "                self.model_b = self.fit_model(model_name_binary, X_train, y_train)\n",
    "                q.put('Done')\n",
    "                p.join()\n",
    "\n",
    "                y_pred = self.model_b.predict(X_test)\n",
    "\n",
    "                y_pred = y_pred.astype(str)\n",
    "                y_test = y_test.astype(str)\n",
    "                y_pred[y_pred == \"0\"] = 'Normal'\n",
    "                y_pred[y_pred == \"1\"] = 'Attack'\n",
    "                y_test[y_test == \"0\"] = 'Normal'\n",
    "                y_test[y_test == \"1\"] = 'Attack'\n",
    "                print(f\"Evaluation of Attack/Normal Classification for {model_name_binary} Algorithm\")\n",
    "                self.evaluate_model(y_test, y_pred, y_bin.unique(), model_name_binary, \"Attack/Normal Classification\")\n",
    "\n",
    "                # Training for Attacks Family Classification\n",
    "                X = df[df['Label'] != 'Normal'].copy(deep=True)\n",
    "                y_f = X['Label']\n",
    "                X = X.drop(['ID', 'Label'], axis=1)\n",
    "                y_f_ = y_f.map({x:i for i,x in enumerate(y_f.unique())})\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y_f_, test_size=0.2, random_state=1) # 80% training and 20% test\n",
    "\n",
    "                # Ressource Check\n",
    "                q = Queue()\n",
    "                p = Process(target=check_ressources, args=(q, f\"Attacks Family Classification Model Used: {model_name_family}\"))\n",
    "                p.start()\n",
    "                self.model_f = self.fit_model(model_name_family, X_train, y_train)\n",
    "                q.put('Done')\n",
    "                p.join()\n",
    "\n",
    "\n",
    "\n",
    "                y_pred = self.model_f.predict(X_test)\n",
    "                y_pred = y_pred.astype(str)\n",
    "                y_test = y_test.astype(str)\n",
    "                for i,x in enumerate(y_f.unique()):\n",
    "                    y_pred[y_pred == str(i)] = x\n",
    "                    y_test[y_test == str(i)] = x\n",
    "\n",
    "                print(f\"Evaluation of Attacks Family Classification Classification for {model_name_family} Algorithm\")\n",
    "                self.evaluate_model(y_test, y_pred, y_f.unique(), model_name_family, \"Attacks Family Classification\")\n",
    "            \n",
    "            display(self.output_evaluation)\n",
    "                    \n",
    "                 \n",
    "            \n",
    "        \n",
    "    def evaluate_model(self, y_test, y_pred, cat, model_name, title):\n",
    "        self.output_evaluation = self.output_evaluation.append({\n",
    "                            'Model':model_name,\n",
    "                            'Precision': metrics.precision_score(y_test, y_pred, average='micro'),\n",
    "                            'Recall': metrics.recall_score(y_test, y_pred, average='micro'),\n",
    "                            'Balanced Accuracy': metrics.balanced_accuracy_score(y_test, y_pred),\n",
    "                            'Matthews Correlation Coefficient': metrics.matthews_corrcoef(y_test, y_pred),\n",
    "                            'AUPRC': auprc_multiclass(y_test, y_pred, cat),\n",
    "                            'Time Taken': self.delta * 1000,\n",
    "                            'Evaluation': title}, ignore_index=True)\n",
    "        print(f\"Precision:{metrics.precision_score(y_test, y_pred, average='micro')}\\n\")\n",
    "        print(f\"Recall:{metrics.recall_score(y_test, y_pred, average='micro')}\\n\")\n",
    "        print(f\"Accuracy:{metrics.accuracy_score(y_test, y_pred)}\\n\")\n",
    "        print(f\"Balanced Accuracy:{metrics.balanced_accuracy_score(y_test, y_pred)}\\n\")\n",
    "        print(f\"Matthews Correlation Coefficient:{metrics.matthews_corrcoef(y_test, y_pred)}\\n\")\n",
    "        print(f\"AUPRC:{auprc_multiclass(y_test, y_pred, cat)}\\n\")\n",
    "        print(f\"Time Taken: {self.delta * 1000} ms\\n\")\n",
    "        plot_confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "    def cross_validation_ga(self, model, X_train,y_train, param_grid):\n",
    "        from sklearn_genetic import GASearchCV\n",
    "        from sklearn_genetic import ExponentialAdapter\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        \n",
    "        # Instantiate the GA search model\n",
    "        mutation_adapter = ExponentialAdapter(initial_value=0.8, end_value=0.2, adaptive_rate=0.1)\n",
    "        crossover_adapter = ExponentialAdapter(initial_value=0.2, end_value=0.8, adaptive_rate=0.1)\n",
    "        cv = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "        evolved_estimator = GASearchCV(estimator=model,\n",
    "                               cv=cv,\n",
    "                               scoring='accuracy',\n",
    "                               population_size=20,\n",
    "                               generations=20,\n",
    "                               mutation_probability=mutation_adapter,\n",
    "                               crossover_probability=crossover_adapter,\n",
    "                               param_grid=param_grid,\n",
    "                               n_jobs=-1)\n",
    "        evolved_estimator.fit(X_train, y_train)\n",
    "        return evolved_estimator\n",
    "    \n",
    "    def fit_model(self, model_name, X_train, y_train):\n",
    "        start = time.time()\n",
    "        if model_name == 'CART':\n",
    "            from sklearn.tree import DecisionTreeClassifier\n",
    "            model = DecisionTreeClassifier(splitter='best', max_depth=8, min_samples_leaf=100)\n",
    "            if self.opt:\n",
    "                from sklearn_genetic.space import Categorical, Integer\n",
    "                param_grid = {\n",
    "                'criterion':Categorical(['gini','entropy']),\n",
    "                'splitter':Categorical(['best','random']),                                        \n",
    "                'min_samples_leaf':Integer(100,300),\n",
    "                'max_depth':Integer(3,8),\n",
    "                }\n",
    "                model = self.cross_validation_ga(model, X_train, y_train, param_grid)\n",
    "                with open(f\"{model_name}-best.txt\", \"a\") as f:\n",
    "                    f.write(f\"{model.best_params_}\\n\\n\\n\")\n",
    "                    f.close()\n",
    "                return model\n",
    "        elif model_name == 'XGBOOST':\n",
    "            import xgboost as xgb\n",
    "            model = xgb.XGBClassifier(objective= 'binary:logistic', n_estimators= 268, max_depth= 8, learning_rate=0.08455776011574645, tree_method='gpu_hist')\n",
    "            if self.opt:\n",
    "                from sklearn_genetic.space import Categorical, Integer, Continuous\n",
    "                param_grid = {\n",
    "                'objective':Categorical(['binary:logistic','multi:softmax']),\n",
    "                'n_estimators':Integer(100,300),\n",
    "                'max_depth':Integer(3,8),\n",
    "                'learning_rate':Continuous(0.001, 0.1),\n",
    "                }\n",
    "                model = self.cross_validation_ga(model, X_train, y_train, param_grid)\n",
    "                with open(f\"{model_name}-best.txt\", \"a\") as f:\n",
    "                    f.write(f\"{model.best_params_}\\n\\n\\n\")\n",
    "                    f.close()\n",
    "                return model\n",
    "        elif model_name == 'SVM':\n",
    "            from sklearn import svm\n",
    "            from sklearn.pipeline import make_pipeline\n",
    "            model = make_pipeline(StandardScaler(), svm.SVC(random_state=0,kernel='linear', decision_function_shape='ovo'))\n",
    "            if self.opt:\n",
    "                model = svm.SVC(random_state=0,kernel='linear', decision_function_shape='ovo')\n",
    "                from sklearn_genetic.space import Categorical, Integer, Continuous\n",
    "                param_grid = {\n",
    "                'kernal':Categorical(['linear', 'poly', 'rbf', 'sigmoid', 'precomputed']),\n",
    "                'gamma':Categorical(['auto','scale']),\n",
    "                'decision_function_shape':Categorical(['ovr','ovo'])\n",
    "                }\n",
    "                model = self.cross_validation_ga(model, StandardScaler(X_train), y_train, param_grid)\n",
    "                with open(f\"{model_name}-best.txt\", \"a\") as f:\n",
    "                    f.write(f\"{model.best_params_}\\n\\n\\n\")\n",
    "                    f.close()\n",
    "                return make_pipeline(StandardScaler(), model)\n",
    "        elif model_name == 'KNN':\n",
    "            from sklearn.neighbors import KNeighborsClassifier\n",
    "            model = KNeighborsClassifier()\n",
    "            if self.opt:\n",
    "                from sklearn_genetic.space import Categorical, Integer, Continuous\n",
    "                param_grid = {\n",
    "                'weights':Categorical(['uniform','distance']),\n",
    "                'n_neighbors':Integer(3,9),\n",
    "                'leaf_size':Integer(20,50),\n",
    "                'algorithm':Categorical(['auto','ball_tree', 'kd_tree', 'brute']),\n",
    "                }\n",
    "                model = self.cross_validation_ga(model, X_train, y_train, param_grid)\n",
    "                with open(f\"{model_name}-best.txt\", \"a\") as f:\n",
    "                    f.write(f\"{model.best_params_}\\n\\n\\n\")\n",
    "                    f.close()\n",
    "                return model\n",
    "        elif model_name == 'RandomForest':\n",
    "            from sklearn.ensemble import RandomForestClassifier\n",
    "            model = RandomForestClassifier()\n",
    "            if self.opt:\n",
    "                from sklearn_genetic.space import Categorical, Integer, Continuous\n",
    "                param_grid = {\n",
    "                'criterion':Categorical(['gini','entropy']),\n",
    "                'min_samples_split':Integer(1,10),\n",
    "                'max_depth':Integer(3,8),\n",
    "                }\n",
    "                model = self.cross_validation_ga(model, X_train, y_train, param_grid)\n",
    "                with open(f\"{model_name}-best.txt\", \"a\") as f:\n",
    "                    f.write(f\"{model.best_params_}\\n\\n\\n\")\n",
    "                    f.close()\n",
    "                return model\n",
    "        elif model_name == 'MLP':\n",
    "            from sklearn.neural_network import MLPClassifier\n",
    "            model = MLPClassifier(random_state=1, max_iter=300)\n",
    "            if self.opt:\n",
    "                from sklearn_genetic.space import Categorical, Integer, Continuous\n",
    "                param_grid = {\n",
    "                'activation':Categorical(['identité', 'logistique', 'tanh', 'relu']),\n",
    "                'solveur':Categorical(['lbfgs', 'sgd', 'adam']),\n",
    "                'max_iter':Integer(100,500),\n",
    "                }\n",
    "                model = self.cross_validation_ga(model, X_train, y_train, param_grid)\n",
    "                with open(f\"{model_name}-best.txt\", \"a\") as f:\n",
    "                    f.write(f\"{model.best_params_}\\n\\n\\n\")\n",
    "                    f.close()\n",
    "                return model\n",
    "        else:\n",
    "            raise ValueError( \"Model not Implemented yet. Please Choose between : CART, RandomForest, KNN, XGBOOST, MLP and SVM\")\n",
    "        model.fit(X_train, y_train)\n",
    "        self.delta = time.time() - start\n",
    "        return model\n",
    "    def is_malware(self, model, malware_features):\n",
    "        return (Boolean, probability)\n",
    "    def pca(self, X, y_bin, y_f, y_sf=None):\n",
    "        from sklearn.decomposition import PCA\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        # Scale data before applying PCA\n",
    "        scaling=StandardScaler()\n",
    "        # Use fit and transform method\n",
    "        scaling.fit(X)\n",
    "        Scaled_X=scaling.transform(X)\n",
    "        principal=PCA()\n",
    "        principal.fit(Scaled_X)\n",
    "        #Let's find how many compenents do we need to preserve 80% of the explained variance\n",
    "        nb_cmp = 0\n",
    "        for i,x in enumerate(principal.explained_variance_ratio_.cumsum()): \n",
    "            if x >= 0.8:\n",
    "                nb_cmp = i\n",
    "                print(next(f'Number of components : {n} , Value of Cumulative Explained Variance: {x}' ))\n",
    "                break\n",
    "        #Visualization\n",
    "        plt.figure(figsize = (15,8))\n",
    "        plt.plot(range(1,len(X.columns)+1), principal.explained_variance_ratio_.cumsum(), linestyle = '--', marker = 'o')\n",
    "        _ = plt.title('Explained Variance by Components')\n",
    "        _ = plt.xlabel('Number of components')\n",
    "        _ = plt.ylabel('Cumulative Explained Variance')\n",
    "        principal=PCA(n_components = nb_cmp)\n",
    "        principal.fit(Scaled_X)\n",
    "        x=principal.transform(Scaled_X)\n",
    "        plt.figure(figsize=(10,10))\n",
    "        ax = plt.axes(projection =\"3d\")\n",
    "        ax.scatter3D(x[:,0],x[:,1],x[:,2],c=y_bin.tolist(),cmap='plasma')\n",
    "        _ = plt.xlabel('First Component')\n",
    "        _ = plt.ylabel('Second Component')\n",
    "        _ = ax.set_zlabel('Third Component')\n",
    "        plt.figure(figsize=(10,10))\n",
    "        ax = plt.axes(projection =\"3d\")\n",
    "        ax.scatter3D(x[:,0],x[:,1],x[:,2],c=y_f.tolist(),cmap='plasma')\n",
    "        _ = plt.xlabel('First Component')\n",
    "        _ = plt.ylabel('Second Component')\n",
    "        _ = ax.set_zlabel('Third Component')\n",
    "        if y_sf is not None:\n",
    "            plt.figure(figsize=(10,10))\n",
    "            ax = plt.axes(projection =\"3d\")\n",
    "            ax.scatter3D(x[:,0],x[:,1],x[:,2],c=y_sf.tolist(),cmap='plasma')\n",
    "            _ = plt.xlabel('First Component')\n",
    "            _ = plt.ylabel('Second Component')\n",
    "            _ = ax.set_zlabel('Third Component')\n",
    "    def tsne(self, X, y_bin, y_f, y_sf=None):\n",
    "        from sklearn.manifold import TSNE\n",
    "        tsne = TSNE(n_components=3, verbose=(1 if self.verbose else 0), random_state=123)\n",
    "        x = tsne.fit_transform(X) \n",
    "        plt.figure(figsize=(10,10))\n",
    "        ax = plt.axes(projection =\"3d\")\n",
    "        ax.scatter3D(x[:,0],x[:,1],x[:,2],c=y_bin.tolist(),cmap='plasma')\n",
    "        _ = plt.xlabel('First Component')\n",
    "        _ = plt.ylabel('Second Component')\n",
    "        _ = ax.set_zlabel('Third Component')\n",
    "        plt.figure(figsize=(10,10))\n",
    "        ax = plt.axes(projection =\"3d\")\n",
    "        ax.scatter3D(x[:,0],x[:,1],x[:,2],c=y_f.tolist(),cmap='plasma')\n",
    "        _ = plt.xlabel('First Component')\n",
    "        _ = plt.ylabel('Second Component')\n",
    "        _ = ax.set_zlabel('Third Component')\n",
    "        if y_sf is not None:\n",
    "            plt.figure(figsize=(10,10))\n",
    "            ax = plt.axes(projection =\"3d\")\n",
    "            ax.scatter3D(x[:,0],x[:,1],x[:,2],c=y_sf.tolist(),cmap='plasma')\n",
    "            _ = plt.xlabel('First Component')\n",
    "            _ = plt.ylabel('Second Component')\n",
    "            _ = ax.set_zlabel('Third Component')\n",
    "    def umap(self, X, y_bin, y_f, y_sf=None):\n",
    "        from umap import UMAP\n",
    "        umap = UMAP(n_components=3, verbose=1, random_state=123)\n",
    "        x = umap.fit_transform(X)\n",
    "        plt.figure(figsize=(10,10))\n",
    "        ax = plt.axes(projection =\"3d\")\n",
    "        ax.scatter3D(x[:,0],x[:,1],x[:,2],c=y_bin.tolist(),cmap='plasma')\n",
    "        _ = plt.xlabel('First Component')\n",
    "        _ = plt.ylabel('Second Component')\n",
    "        _ = ax.set_zlabel('Third Component')\n",
    "        plt.figure(figsize=(10,10))\n",
    "        ax = plt.axes(projection =\"3d\")\n",
    "        ax.scatter3D(x[:,0],x[:,1],x[:,2],c=y_f.tolist(),cmap='plasma')\n",
    "        _ = plt.xlabel('First Component')\n",
    "        _ = plt.ylabel('Second Component')\n",
    "        _ = ax.set_zlabel('Third Component')\n",
    "        if y_sf is not None:\n",
    "            plt.figure(figsize=(10,10))\n",
    "            ax = plt.axes(projection =\"3d\")\n",
    "            ax.scatter3D(x[:,0],x[:,1],x[:,2],c=y_sf.tolist(),cmap='plasma')\n",
    "            _ = plt.xlabel('First Component')\n",
    "            _ = plt.ylabel('Second Component')\n",
    "            _ = ax.set_zlabel('Third Component')\n",
    "        \n",
    "    def visualization(self, method='PCA'):\n",
    "        print(f\"Visualization of the dataset {self.dataset} in progress ... Method: {method}\")\n",
    "        if self.dataset == 'MalMen2020':\n",
    "            plot_cols = ['Category_Family', 'Category_Subfamily']\n",
    "            plot_names = ['Category Family', 'Category Subfamily'] \n",
    "            for c,n in zip(plot_cols, plot_names):\n",
    "                fig = plt.figure(figsize=(30, 8))\n",
    "                ax = sns.countplot(data=self.df, x=c)\n",
    "                ax.set_title(f'Repartition of {n} in {self.dataset} Dataset', fontsize=20)\n",
    "                _ = plt.xticks(rotation = 90, fontsize=18)\n",
    "                _ = plt.xlabel(n,fontsize=14)\n",
    "                _ = plt.ylabel(\"Count\", fontsize=14)\n",
    "                \n",
    "            X = self.df.copy(deep=True)\n",
    "            y_bin = X['Class']\n",
    "            y_f = X['Category_Family']\n",
    "            y_sf = X['Category_Subfamily']\n",
    "            X = X.drop(['Class', 'Category_Subfamily', 'Category_Family'], axis=1)\n",
    "            y_bin = y_bin.replace(['Benign', 'Malware'], ['green', 'red'])\n",
    "            y_f = y_f.replace(['Benign', 'Ransomware' ,'Spyware', 'Trojan'], ['green', 'yellow', 'orange', 'red'])\n",
    "            y_sf = y_sf.replace(y_sf.unique(), [x for x in range(len(y_sf.unique()))])\n",
    "            if method == 'PCA':\n",
    "                self.pca(X, y_bin, y_f, y_sf)\n",
    "            elif method == 'TSNE':\n",
    "                self.tsne(X, y_bin, y_f, y_sf)\n",
    "            elif method == 'UMAP':\n",
    "                self.umap(X, y_bin, y_f, y_sf)\n",
    "            else:\n",
    "                raise ValueError( \"Only PCA, TSNE and UMAP are implemented make sure to choose only one of them.\")\n",
    "        else:\n",
    "            for i, df in enumerate(self.df):\n",
    "                fig = plt.figure(figsize=(30, 8))\n",
    "                ax = sns.countplot(data=df, x='Label')\n",
    "                ax.set_title(f'Repartition of Normal and Attack Labels in {self.dataset} Dataset: {self.type_g[i]}', fontsize=20)\n",
    "                _ = plt.xticks(rotation = 90, fontsize=18)\n",
    "                _ = plt.xlabel('Labels',fontsize=14)\n",
    "                _ = plt.ylabel(\"Count\", fontsize=14)\n",
    "                \n",
    "                fig = plt.figure(figsize=(30, 8))\n",
    "                ax = sns.countplot(data=df[df['Label'] != \"Normal\"], x='Label')\n",
    "                ax.set_title(f'Repartition of Attack Labels in {self.dataset} Dataset: {self.type_g[i]}', fontsize=20)\n",
    "                _ = plt.xticks(rotation = 90, fontsize=18)\n",
    "                _ = plt.xlabel('Attack Labels',fontsize=14)\n",
    "                _ = plt.ylabel(\"Count\", fontsize=14)\n",
    "\n",
    "                X = df.copy(deep=True)\n",
    "                y_bin = X['Label']\n",
    "                y_bin = y_bin.map({x:\"green\" if x == \"Normal\" else \"red\" for x in y_bin.unique()})\n",
    "                y_f = X['Label']\n",
    "                X = X.drop(['Label'], axis=1)\n",
    "                y_f = y_f.replace(y_f.unique(), [x for x in range(len(y_f.unique()))])\n",
    "                if method == 'PCA':\n",
    "                    self.pca(X, y_bin, y_f)\n",
    "                elif method == 'TSNE':\n",
    "                    self.tsne(X, y_bin, y_f)\n",
    "                elif method == 'UMAP':\n",
    "                    self.umap(X, y_bin, y_f)\n",
    "                else:\n",
    "                    raise ValueError( \"Only PCA, TSNE and UMAP are implemented make sure to choose only one of them.\")\n",
    "        plt.show()\n",
    "            \n",
    "        \n",
    "        \n",
    "    def preprocessing(self, df):\n",
    "        if self.verbose:\n",
    "            # Exploring the Dataset:\n",
    "            print(df.head())\n",
    "            col = list(df.columns)\n",
    "            print(f\"Column number: {len(col)}\\n\")\n",
    "            print(f\"Column names: {col}\\n\")\n",
    "            print(f\"Column types: \\n{df.dtypes}\\n\")\n",
    "            print(f\"Number of rows of the data set: {len(df)}\\n\")\n",
    "            cat_col = df.select_dtypes('object')\n",
    "            print(f\"Categorial columns: \\n{cat_col.columns}\\n\\n\")\n",
    "            for c in cat_col:\n",
    "                print(f\"Existing values of {c}: {cat_col[c].unique()}\")\n",
    "\n",
    "            num_col = df.select_dtypes(include=np.number)\n",
    "            print(f\"Numerical columns: \\n{num_col.columns}\\n\\n\")\n",
    "            for c in num_col:\n",
    "                print(f\"Existing values of {c}: {num_col[c].unique()}\")\n",
    "                print(f\"Min Value of {c}: {np.min(num_col[c])}\")\n",
    "                print(f\"Max Value of {c}: {np.max(num_col[c])}\")\n",
    "                print(f\"Std deviation Value of {c}: {np.std(num_col[c])}\")\n",
    "            if self.dataset == \"MalMen2020\":\n",
    "                lookup_query = df.query('Class == \"Malware\" & Category != \"Benign\"').index\n",
    "                print(f\"Checking if the positive class (1) match malware categories : {len(lookup_query) > 0}\")\n",
    "\n",
    "                lookup_query = df.query('Category != \"Benign\" & Class == \"Malware\"').index\n",
    "                print(f\"Checking if the malware categories match Class : {len(lookup_query) > 0}\")\n",
    "\n",
    "                lookup_query = df.query('Class == \"Benign\" & Category == \"Benign\"').index\n",
    "                print(f\"Checking if the begnin class match the ‘Benign’ malware category : {len(lookup_query) > 0}\")\n",
    "                print(f\"Number of malware class : {len(df[df['Class'] == 'Malware'])}, ratio : {(len(df[df['Class'] == 'Malware'])/len(df))*100:.2f}%\")\n",
    "                print(f\"Number of benign class : {len(df[df['Class'] == 'Benign'])}, ratio : {(len(df[df['Class'] == 'Benign'])/len(df))*100:.2f}%\")\n",
    "            else:\n",
    "                print(f\"Number of malware class : {len(df[df['Label'] != 'Normal'])}, ratio : {(len(df[df['Label'] != 'Normal'])/len(df))*100:.2f}%\")\n",
    "                print(f\"Number of benign class : {len(df[df['Label'] == 'Normal'])}, ratio : {(len(df[df['Label'] == 'Normal'])/len(df))*100:.2f}%\")\n",
    "                \n",
    "        \n",
    "        # Data Cleaning\n",
    "        column_to_remove = []\n",
    "        for c in df.columns:\n",
    "            if len(df[c].unique()) == 1:\n",
    "                column_to_remove.append(c)\n",
    "        df = df.drop(column_to_remove, axis=1)\n",
    "        if self.dataset == \"MalMen2020\":\n",
    "            cat = df['Category'].tolist()\n",
    "            cat_f = []\n",
    "            cat_sf = []\n",
    "            for x in cat:\n",
    "                f_sf = x.split('-')\n",
    "                if len(f_sf) >= 3:\n",
    "                    cat_f.append(f_sf[0])\n",
    "                    cat_sf.append(f_sf[0] + \"-\" + f_sf[1])\n",
    "                elif len(f_sf) >= 2:\n",
    "                    cat_f.append(f_sf[0])\n",
    "                    cat_sf.append(f_sf[0])\n",
    "                elif len(f_sf) >= 1:\n",
    "                    cat_f.append(x)\n",
    "                    cat_sf.append(x)\n",
    "                else:\n",
    "                    raise ValueError( f\"Error : There is no Family in {x}\")\n",
    "            df['Category_Family'] = cat_f\n",
    "            df['Category_Subfamily'] = cat_sf\n",
    "            df = df.drop(['Category'], axis=1)\n",
    "            if self.verbose:\n",
    "                print(f\"Number of occurrences for each malware class: \\n{df['Category_Family'].value_counts()}\")\n",
    "                print(f\"Number of occurrences for each malware class: \\n{df['Category_Subfamily'].value_counts()}\")\n",
    "            self.df = df\n",
    "        else:\n",
    "            df = df.drop(['Unnamed: 0'], axis=1)\n",
    "            if self.df is None:\n",
    "                self.df = [df]\n",
    "            else:\n",
    "                self.df.append(df)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddb6bfba",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Obfuscated-MalMem2022.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m md \u001b[38;5;241m=\u001b[39m \u001b[43mMalware_Detection\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m md\u001b[38;5;241m.\u001b[39minit_model()\n",
      "Cell \u001b[0;32mIn[7], line 81\u001b[0m, in \u001b[0;36mMalware_Detection.__init__\u001b[0;34m(self, dataset, verbose, optimization, lines)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMalMen2020\u001b[39m\u001b[38;5;124m\"\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, optimization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dataset \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMalMen2020\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 81\u001b[0m         dfs \u001b[38;5;241m=\u001b[39m [\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mObfuscated-MalMem2022.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m dataset \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mADFA-LD\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_g \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3-gramme\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5-gramme\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m6-gramme\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Obfuscated-MalMem2022.csv'"
     ]
    }
   ],
   "source": [
    "md = Malware_Detection(optimization=False, lines=-1, verbose=False)\n",
    "md.init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f4f118",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5031c11c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
